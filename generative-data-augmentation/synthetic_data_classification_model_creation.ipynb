{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK8mdu2YzWfs"
   },
   "source": [
    "## Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGXtk6BezTqb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS3LvY3Sz4hU",
    "outputId": "66935461-6d2c-4128-d652-fcbae86cbc7b"
   },
   "outputs": [],
   "source": [
    "#mount drive\n",
    "%cd ..\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "\n",
    "# list the contents of /mydrive\n",
    "!ls /mydrive\n",
    "\n",
    "#Navigate to /mydrive/yolov4\n",
    "%cd /mydrive/Ocular_Disease/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "670C4UHAznXd"
   },
   "outputs": [],
   "source": [
    "# Set the path to the directory containing your data\n",
    "data_dir = '/content/gdrive/MyDrive/Ocular_Disease/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w-8NJKVwc1o"
   },
   "source": [
    "## Finding STD and MEAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQWH5xgo_1dO"
   },
   "source": [
    "Take and save pictures one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGdYPkZppoHR"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Get the list of all class labels (folders) from the dataset directory\n",
    "class_labels = os.listdir(data_dir)\n",
    "\n",
    "# Initialize an empty list to hold the dataset\n",
    "dataset = []\n",
    "\n",
    "# Loop through each class label\n",
    "for class_label in class_labels:\n",
    "    # Construct the path to the class directory\n",
    "    class_dir = os.path.join(data_dir, class_label)\n",
    "    \n",
    "    # Skip if the current path is not a directory (this ensures we only process folders)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # Get the list of image files in the class directory\n",
    "    image_files = os.listdir(class_dir)\n",
    "\n",
    "    # Loop through each image file in the class directory\n",
    "    for image_file in image_files:\n",
    "        # Construct the full path to the image file\n",
    "        image_path = os.path.join(class_dir, image_file)\n",
    "        \n",
    "        # Open the image, convert it to RGB (to ensure it's in the right format)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "        # Append the image and its corresponding class label as a tuple to the dataset\n",
    "        dataset.append((image, class_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c86GXnXorLzB",
    "outputId": "020e9b3b-cfc5-4700-cb78-d52917bb7a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [112.121926    71.96017866  38.72672353]\n",
      "Standard Deviation: [75.27826614 54.42092274 37.71428029]\n"
     ]
    }
   ],
   "source": [
    "# Initialize arrays to store the mean and standard deviation for each channel (RGB)\n",
    "mean = np.zeros(3)  # To store mean for each RGB channel\n",
    "std = np.zeros(3)   # To store standard deviation for each RGB channel\n",
    "count = 0\n",
    "\n",
    "# Iterate over the dataset and compute the mean\n",
    "for image, _ in dataset:\n",
    "    # Convert the image from PIL format to a NumPy array\n",
    "    image_array = np.array(image)  \n",
    "    height, width, _ = image_array.shape  # Get the image dimensions (height, width, channels)\n",
    "    \n",
    "    # Reshape the image array to a 2D array (height * width, 3) for easier processing of channels\n",
    "    reshaped_image_array = image_array.reshape(height * width, 3)\n",
    "    \n",
    "    # Calculate the mean across each channel (R, G, B) and add it to the running mean\n",
    "    mean += np.mean(reshaped_image_array, axis=0)\n",
    "    \n",
    "    count += 1  # Increment the image counter\n",
    "\n",
    "# Divide the sum of means by the number of images to get the average mean for each channel\n",
    "mean /= count\n",
    "\n",
    "# Iterate over the dataset again to compute the standard deviation\n",
    "for image, _ in dataset:\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    height, width, _ = image_array.shape  # Get the dimensions of the image\n",
    "    reshaped_image_array = image_array.reshape(height * width, 3)  # Reshape the image array\n",
    "    \n",
    "    # Calculate the squared difference from the mean for each pixel and channel\n",
    "    std += np.mean((reshaped_image_array - mean) ** 2, axis=0)\n",
    "\n",
    "# Compute the square root of the variance (std) to get the standard deviation\n",
    "std = np.sqrt(std / count)\n",
    "\n",
    "# Output the computed mean and standard deviation for each RGB channel\n",
    "print('Mean:', mean)\n",
    "print('Standard Deviation:', std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi8k7qozrRBZ"
   },
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pluNKNRZzm4M"
   },
   "outputs": [],
   "source": [
    "# Define the transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    # Resize the image to a target size of (224, 224) pixels\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Convert the image to a tensor (this converts the image from PIL to a tensor format)\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalize the image tensor with pre-defined mean and standard deviation values\n",
    "    # These values are commonly used for models like ResNet that are pre-trained on ImageNet\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcsgPCKBzmDW"
   },
   "outputs": [],
   "source": [
    "# Create the ImageFolder dataset\n",
    "dataset = ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "print(\"Number of samples in the training set:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8_m1Q0X_kxQ"
   },
   "source": [
    "## Data Augmentation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhydPUednVQz"
   },
   "outputs": [],
   "source": [
    "# Define the transforms for data augmentation\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224), # Apply random resized crop to the image, cropping and resizing it to 224x224 pixels\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally with a probability of 0.5\n",
    "    transforms.RandomRotation(10), # Apply random rotation between -10 and 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1), # Apply random adjustments to brightness, contrast, saturation, and hue\n",
    "    transforms.ToTensor(), # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize the image using the mean and std of ImageNet\n",
    "])\n",
    "\n",
    "# Apply the defined augmentation transforms to the dataset using ImageFolder\n",
    "augmented_dataset = ImageFolder(data_dir, transform=augmentation_transform)\n",
    "\n",
    "print(\"Number of samples in the training set:\", len(augmented_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMCkKNMp_oJS"
   },
   "source": [
    "## Combining Two Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLifkjw8o4DS"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Combine the original dataset and the augmented dataset\n",
    "combined_dataset = ConcatDataset([dataset, augmented_dataset])\n",
    "\n",
    "print(\"Number of samples in the training set:\", len(combined_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DhYsbCx_UG1"
   },
   "source": [
    "### Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99nCg0Vfo7Rj",
    "outputId": "7bfe5279-84dd-4153-ed89-afb19902fcdd"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(combined_dataset, test_size=val_ratio, random_state=42)\n",
    "\n",
    "# Optionally, you can print the number of samples in each set\n",
    "print(\"Number of samples in the training set:\", len(train_dataset))\n",
    "print(\"Number of samples in the validation set:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYbBkW8h_WMz"
   },
   "source": [
    "### Normal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIN0Qr491vKy",
    "outputId": "69d374c2-56de-4446-b462-7b24789aeaf2"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=val_ratio, random_state=42)\n",
    "\n",
    "# Optionally, you can print the number of samples in each set\n",
    "print(\"Number of samples in the training set:\", len(train_dataset))\n",
    "print(\"Number of samples in the validation set:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-E2hi5m_tMS"
   },
   "source": [
    "### BatchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFh80IHVzlbf"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for training, validation, and test sets\n",
    "batch_size = 64  # Adjust the batch size according to your needs\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzC_cSznA1UQ"
   },
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-kmka5uA28w"
   },
   "outputs": [],
   "source": [
    "# Get the unique class names from the files\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVDEctMPA3wl"
   },
   "outputs": [],
   "source": [
    "# Load the pretrained MobileNet model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vtaVB0rCWHp"
   },
   "outputs": [],
   "source": [
    "# Modify the final classifier to match the number of classes (diseases)\n",
    "model.classifier = nn.Sequential(\n",
    "    # Dropout layer with 0.2 probability to help prevent overfitting\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    # Linear layer that maps from 1280 features (e.g., from EfficientNet) to 'num_classes' outputs\n",
    "    nn.Linear(1280, num_classes)  # Adjust 1280 if your model has a different feature size before classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MKxtEudCmfm"
   },
   "outputs": [],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKztbbW1C3ho"
   },
   "outputs": [],
   "source": [
    "# Move the model to the desired device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNnk3bFEA6P-"
   },
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpY1575czkxL",
    "outputId": "9d05f108-c2e9-43c1-a5fc-7779127ed940"
   },
   "outputs": [],
   "source": [
    "# Load the pretrained VGG19 model\n",
    "model = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjzxglB4zkEk"
   },
   "outputs": [],
   "source": [
    "# Freeze the parameters of all layers except the final classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQo5F1kEzjdU"
   },
   "outputs": [],
   "source": [
    "# Replace the final fully connected layers for disease classification\n",
    "num_classes = len(dataset.classes)\n",
    "model.classifier = nn.Sequential(\n",
    "    # First fully connected layer, reducing 25088 input features (e.g., from VGG) to 4096\n",
    "    nn.Linear(25088, 4096),  # Adjust the input size (25088) if needed based on your model architecture\n",
    "    nn.ReLU(),  # ReLU activation for introducing non-linearity\n",
    "    nn.Dropout(p=0.5),  # Dropout layer for regularization to avoid overfitting\n",
    "    \n",
    "    # Second fully connected layer, from 4096 to 4096 features (same size)\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),  # Another ReLU activation\n",
    "    nn.Dropout(p=0.5),  # Dropout again to help with generalization\n",
    "\n",
    "    # Final fully connected layer that maps 4096 features to the number of classes\n",
    "    nn.Linear(4096, num_classes)  # 'num_classes' corresponds to the number of disease classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlOyMHjHzht9"
   },
   "outputs": [],
   "source": [
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTIwU8t9_INe"
   },
   "source": [
    "### Class Imbalance Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj1NhIc97Ib1"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have a PyTorch dataset called 'dataset' containing your data\n",
    "\n",
    "# Compute class weights\n",
    "class_counts = [350, 200, 101, 296, 140, 90, 468, 150]  # Example class counts (replace with your own)\n",
    "total_samples = sum(class_counts)\n",
    "class_weights = [total_samples / (len(class_counts) * count) for count in class_counts]\n",
    "class_weights = torch.Tensor(class_weights).to(device)  # Move class weights to the desired device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-oUl6AU7WdT"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights))\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)  # Replace with your chosen optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR4dCSvZ5yld"
   },
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByylX_T7-Qqt"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE-Obgti5wLL"
   },
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGNz7BmO5vJa"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUOL0Y2q51AC"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "P2y9Sst6zfv9",
    "outputId": "2ee2b11d-ce5b-40db-99d9-cb7b1b02031d"
   },
   "outputs": [],
   "source": [
    "# Training loop parameters\n",
    "num_epochs = 100  \n",
    "train_loss_list = []  \n",
    "val_loss_list = []    \n",
    "train_acc_list = []   \n",
    "val_acc_list = []     \n",
    "\n",
    "# Get the total number of steps in the train and validation dataloaders\n",
    "total_train_steps = len(train_dataloader)\n",
    "total_val_steps = len(val_dataloader)\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    # Loop over the training data\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        images = images.to(device)  # Move images to the GPU\n",
    "        labels = labels.to(device)  # Move labels to the GPU\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)  # Get model predictions\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update model weights\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        train_loss += loss.item()  # Add loss for the current batch\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "        train_total += labels.size(0)  # Total number of labels in this batch\n",
    "        train_correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "        # Print training progress every 10 steps\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_train_steps}], '\n",
    "                  f'Training Loss: {train_loss / (i + 1):.4f}')\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    train_loss /= total_train_steps\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking for validation\n",
    "        for i, (images, labels) in enumerate(val_dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            loss = criterion(outputs, labels)  # Calculate the validation loss\n",
    "\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "            val_total += labels.size(0)  # Total number of labels in this batch\n",
    "            val_correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    val_loss /= total_val_steps\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    # Store the metrics for later use\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, '\n",
    "          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sZRZgDAze8U"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting training and validation results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_loss_list, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_acc_list, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), val_acc_list, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QmzDGUJ2E1N"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), './fine_tuned_vgg19.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czJ7ByBgzUSb"
   },
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqrHbsxgzTGj"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEpkx3PpztYU",
    "outputId": "82889eb4-e32f-4a2e-d3ee-b19a0eda747c"
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned VGG19 model\n",
    "model = vgg19(pretrained=False)  # Initialize VGG19 without pretrained weights\n",
    "num_classes = 8\n",
    "\n",
    "# Modify the classifier to match the number of classes for the task (8 in this case)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 4096),  # Fully connected layer with input size 25088 and output size 4096\n",
    "    nn.ReLU(),  # ReLU activation function\n",
    "    nn.Dropout(p=0.5),  # Dropout layer with 50% probability for regularization\n",
    "    nn.Linear(4096, 4096),  # Fully connected layer with input size 4096 and output size 4096\n",
    "    nn.ReLU(),  # ReLU activation function\n",
    "    nn.Dropout(p=0.5),  # Dropout layer with 50% probability for regularization\n",
    "    nn.Linear(4096, num_classes)  # Final fully connected layer to match the number of output classes\n",
    ")\n",
    "\n",
    "# Load the fine-tuned weights from a saved checkpoint\n",
    "model.load_state_dict(torch.load('./fine_tuned_vgg19.pth'))\n",
    "\n",
    "# Set the model to evaluation mode (disables dropout and batch normalization for inference)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEOwEjw6zrZu"
   },
   "outputs": [],
   "source": [
    "# Define the transformations to be applied to the input image\n",
    "transform = transforms.Compose([  # Combine multiple transformations into a single pipeline\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels (common input size for VGG19)\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image using the mean and std of ImageNet\n",
    "    # These normalization values are typical for pre-trained models like VGG19\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH9MPnjcFrj0"
   },
   "source": [
    "### To check the image by image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXAED29dzrfe"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the test image\n",
    "image_path = '/content/gdrive/MyDrive/Ocular_Disease/classes/image.jpg'  # Replace with the path to your test image\n",
    "image = Image.open(image_path).convert('RGB')  # Open the image and convert it to RGB mode\n",
    "image_tensor = transform(image).unsqueeze(0)  # Apply the defined transformations and add a batch dimension (unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOmqc1cwzq61"
   },
   "outputs": [],
   "source": [
    "# Perform the inference\n",
    "with torch.no_grad():  # Disable gradient calculation as it's not needed for inference\n",
    "    outputs = model(image_tensor)  # Perform forward pass through the model to get raw outputs\n",
    "    _, predicted = torch.max(outputs, 1)  # Get the index of the class with the highest score (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJhJN2d-zqav"
   },
   "outputs": [],
   "source": [
    "# Map the predicted class index to the corresponding class label\n",
    "predicted_label = class_labels[predicted.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtePbh7Fzp0k"
   },
   "outputs": [],
   "source": [
    "# Print the predicted label\n",
    "print('Predicted class:', predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGHXrje7Fnne"
   },
   "source": [
    "### To check the whole file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6skiXKtnI2hh"
   },
   "outputs": [],
   "source": [
    "class_labels = ['OTHER', 'age-related macular degeneration', 'cataract', 'diabetic_retinopathy', 'glaucoma', 'hypertensive retinopathy', 'normal_fundus', 'pathological myopia' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVDHKOURCMkd",
    "outputId": "c5b67d30-ac0a-4ca9-a0b1-ed3a74fc54b4"
   },
   "outputs": [],
   "source": [
    "# Set the path to the directory containing your test images\n",
    "test_dir = '/content/gdrive/MyDrive/Ocular_Disease/outputs/diabetes'  # Replace with the path to your test directory\n",
    "\n",
    "# Get a list of image file names in the test directory\n",
    "image_files = os.listdir(test_dir)\n",
    "\n",
    "# Perform inference for each image in the directory\n",
    "for image_file in image_files:\n",
    "    # Load and preprocess the test image\n",
    "    image_path = os.path.join(test_dir, image_file)  # Construct the full path to the image file\n",
    "    image = Image.open(image_path).convert('RGB')  # Open and convert the image to RGB format\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Apply transformations and add batch dimension\n",
    "\n",
    "    # Perform the inference\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(image_tensor)  # Perform a forward pass through the model to get predictions\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the class with the highest score (prediction)\n",
    "\n",
    "    # Map the predicted class index to the corresponding class label\n",
    "    predicted_label = class_labels[predicted.item()]  # Convert the index to the class label\n",
    "\n",
    "    print(f'Image: {image_file}, Predicted class: {predicted_label}')  # Output the result for each image"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7w-8NJKVwc1o",
    "ewhwN9ggsO8I",
    "SzC_cSznA1UQ",
    "SH9MPnjcFrj0",
    "GMWWhpFrFiEL"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
