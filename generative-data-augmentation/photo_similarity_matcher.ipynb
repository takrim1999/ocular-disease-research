{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Buoj1f7E6p3i",
    "outputId": "374b71c0-72b8-49cd-dc74-089e828098d4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wA3Qmog6vBp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define folder paths for reference images and target images\n",
    "reference_image_folder = '/content/drive/MyDrive/Ocular_Disease/classes/pathological_myopia' # real image\n",
    "target_image_folder = '/content/drive/MyDrive/Ocular_Disease/classified_output/Myopia' # synthetic (generated) image\n",
    "\n",
    "# Load all images from both folders\n",
    "reference_images = [os.path.join(reference_image_folder, img) for img in os.listdir(reference_image_folder)]\n",
    "target_images = [os.path.join(target_image_folder, img) for img in os.listdir(target_image_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQZJSrh46zvq"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained CLIP model and processor from HuggingFace\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")  # Load the CLIP model (Vision Transformer)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")  # Load the CLIP processor for image preprocessing\n",
    "\n",
    "# Function to extract features from an image\n",
    "def get_image_features(image_path):\n",
    "    \"\"\"\n",
    "    This function takes an image file path, processes the image,\n",
    "    and extracts its feature representation using the CLIP model.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "    - features (tensor): The image feature representation as a tensor.\n",
    "    \"\"\"\n",
    "    # Open and convert the image to RGB format (in case it's not in RGB)\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Preprocess the image and return it as a tensor suitable for the CLIP model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Use the CLIP model to extract features without tracking gradients (for efficiency)\n",
    "    with torch.no_grad():\n",
    "        features = model.get_image_features(**inputs)  # Extract image features\n",
    "    \n",
    "    # Return the extracted features\n",
    "    return features\n",
    "\n",
    "# Dictionary to store the top 10 matches for each reference image\n",
    "top_10_matches = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPocBg289w9v"
   },
   "outputs": [],
   "source": [
    "# Compare each reference image with all target images\n",
    "for ref_image in reference_images:  # Loop over each reference image\n",
    "    # Extract features of the reference image\n",
    "    reference_features = get_image_features(ref_image)  \n",
    "\n",
    "    similarities = []  # List to store similarity scores between reference and target images\n",
    "    \n",
    "    for target_image in target_images:  # Loop over each target image\n",
    "        # Extract features of the target image\n",
    "        target_features = get_image_features(target_image)  \n",
    "        \n",
    "        # Compute cosine similarity between the reference and target image features\n",
    "        similarity = torch.cosine_similarity(reference_features, target_features)\n",
    "        \n",
    "        # Append the target image and its similarity score to the list\n",
    "        similarities.append((target_image, similarity.item()))\n",
    "\n",
    "    # Sort the target images by their similarity score in descending order\n",
    "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # Store the top 10 most similar images for each reference image\n",
    "    top_10_matches[ref_image] = sorted_similarities\n",
    "\n",
    "# Print the top 10 matches for each reference image\n",
    "for ref_image, matches in top_10_matches.items():  # Loop over reference images and their top matches\n",
    "    print(f\"Reference image: {ref_image}\")  # Print the reference image\n",
    "    for idx, (best_match, similarity) in enumerate(matches):  # Loop over the top matches\n",
    "        print(f\"Top {idx+1}: {best_match}, Similarity: {similarity}\")  # Print each match with its similarity score\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
