{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Try to import tensorflow_addons, if not available, define alternative metrics\n",
        "try:\n",
        "    import tensorflow_addons as tfa\n",
        "    HAS_TFA = True\n",
        "except ImportError:\n",
        "    print(\"Warning: tensorflow_addons not available. Using alternative F1 score implementation.\")\n",
        "    HAS_TFA = False\n",
        "    \n",
        "    # Define a simple F1 score metric as alternative\n",
        "    class F1Score(tf.keras.metrics.Metric):\n",
        "        def __init__(self, num_classes, average='weighted', name='f1', **kwargs):\n",
        "            super(F1Score, self).__init__(name=name, **kwargs)\n",
        "            self.num_classes = num_classes\n",
        "            self.average = average\n",
        "            self.precision = tf.keras.metrics.Precision()\n",
        "            self.recall = tf.keras.metrics.Recall()\n",
        "            \n",
        "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "            y_pred = tf.argmax(y_pred, axis=1)\n",
        "            y_true = tf.argmax(y_true, axis=1)\n",
        "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "            \n",
        "        def result(self):\n",
        "            p = self.precision.result()\n",
        "            r = self.recall.result()\n",
        "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
        "            \n",
        "        def reset_state(self):\n",
        "            self.precision.reset_state()\n",
        "            self.recall.reset_state()\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# GPU Configuration and Memory Management\n",
        "print(\"🔧 Configuring GPU and memory settings...\")\n",
        "\n",
        "# Check GPU availability\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"✅ GPU is available! 🚀\")\n",
        "    print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
        "    \n",
        "    # Configure GPU memory growth to avoid allocating all memory at once\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"✅ GPU memory growth enabled\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"⚠️ GPU configuration error: {e}\")\n",
        "else:\n",
        "    print(\"⚠️ GPU not available. Using CPU (will be slower)\")\n",
        "    print(\"💡 Consider using Kaggle or Google Colab for GPU access\")\n",
        "\n",
        "print(\"🔧 Configuration complete!\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Prepared Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the ODIR dataset from Kaggle\n",
        "# Dataset structure: /kaggle/input/ocular-disease-recognition-odir5k/\n",
        "data_path = \"/kaggle/input/ocular-disease-recognition-odir5k/\"\n",
        "images_path = os.path.join(data_path, \"preprocessed_images\")\n",
        "\n",
        "# Load the dataset CSV file\n",
        "df = pd.read_csv(os.path.join(data_path, \"full_df.csv\"))\n",
        "print(f\"Dataset loaded. Total samples: {len(df)}\")\n",
        "\n",
        "# Display basic dataset information\n",
        "print(\"\\nDataset columns:\", df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check if we have preprocessed train/val/test splits\n",
        "# If not available, we'll create them from the dataset\n",
        "train_split_path = os.path.join(data_path, \"x_train.npy\")\n",
        "if os.path.exists(train_split_path):\n",
        "    print(\"\\nLoading preprocessed train/val/test splits...\")\n",
        "    x_train = np.load(os.path.join(data_path, \"x_train.npy\"))\n",
        "    y_train = np.load(os.path.join(data_path, \"y_train.npy\"))\n",
        "    x_val = np.load(os.path.join(data_path, \"x_val.npy\"))\n",
        "    y_val = np.load(os.path.join(data_path, \"y_val.npy\"))\n",
        "    x_test = np.load(os.path.join(data_path, \"x_test.npy\"))\n",
        "    y_test = np.load(os.path.join(data_path, \"y_test.npy\"))\n",
        "    print(\"Preprocessed splits loaded successfully!\")\n",
        "else:\n",
        "    print(\"\\nPreprocessed splits not found. Will create them from the dataset.\")\n",
        "    # We'll create the splits in the next cell\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = None, None, None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train/val/test splits from ODIR dataset if not preprocessed\n",
        "if x_train is None:\n",
        "    print(\"Creating train/val/test splits from ODIR dataset...\")\n",
        "    \n",
        "    # Data preprocessing\n",
        "    df[\"class\"] = df[\"labels\"].apply(lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
        "    \n",
        "    # Filter for the 5 main classes we want to use\n",
        "    target_classes = [\"G\", \"C\", \"A\", \"H\", \"M\"]  # Glaucoma, Cataract, AMD, Hypertension, Myopia\n",
        "    df_filtered = df[df[\"class\"].isin(target_classes)]\n",
        "    print(f\"Filtered dataset for 5 classes: {len(df_filtered)} samples\")\n",
        "    \n",
        "    # Create image dataset\n",
        "    def create_dataset(img_list, class_label, max_images=None):\n",
        "        dataset = []\n",
        "        count = 0\n",
        "        \n",
        "        for img in img_list:\n",
        "            if max_images is not None and count >= max_images:\n",
        "                break\n",
        "            \n",
        "            image_path = os.path.join(images_path, img)\n",
        "            if not os.path.exists(image_path):\n",
        "                continue\n",
        "                \n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                continue\n",
        "            \n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = cv2.resize(image, (224, 224))\n",
        "            dataset.append([np.array(image), class_label])\n",
        "            count += 1\n",
        "        \n",
        "        return dataset\n",
        "    \n",
        "    # Define class mappings\n",
        "    class_short2full = {\n",
        "        \"G\": \"Glaucoma\",  # \"G\" is mapped to \"Glaucoma\"\n",
        "        \"C\": \"Cataract\",  # \"C\" is mapped to \"Cataract\"\n",
        "        \"A\": \"Age Related Macular Degeneration\",  # \"A\" is mapped to \"Age Related Macular Degeneration\"\n",
        "        \"H\": \"Hypertension\",  # \"H\" is mapped to \"Hypertension\"\n",
        "        \"M\": \"Myopia\"  # \"M\" is mapped to \"Myopia\"\n",
        "    }\n",
        "    \n",
        "    # Dictionary to map short class labels to integer class indices\n",
        "    class_dict = {\n",
        "        \"G\": 0,  # \"G\" is mapped to index 0\n",
        "        \"C\": 1,  # \"C\" is mapped to index 1\n",
        "        \"A\": 2,  # \"A\" is mapped to index 2\n",
        "        \"H\": 3,  # \"H\" is mapped to index 3\n",
        "        \"M\": 4   # \"M\" is mapped to index 4\n",
        "    }\n",
        "    \n",
        "    # Build dataset for each class\n",
        "    dataset = []\n",
        "    for class_ in target_classes:\n",
        "        img_list = df_filtered.loc[df_filtered[\"class\"] == class_][\"filename\"].values\n",
        "        class_label = class_dict[class_]\n",
        "        class_dataset = create_dataset(img_list, class_label, max_images=None)\n",
        "        dataset += class_dataset\n",
        "        print(f\"Class {class_short2full[class_]}: {len(class_dataset)} samples\")\n",
        "    \n",
        "    # Shuffle and split\n",
        "    random.shuffle(dataset)\n",
        "    \n",
        "    # Convert to arrays\n",
        "    X = np.array([i[0] for i in dataset])\n",
        "    y = np.array([i[1] for i in dataset])\n",
        "    \n",
        "    # Split the data\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    \n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
        "    \n",
        "    # Convert to categorical\n",
        "    y_train = to_categorical(y_train, 5)\n",
        "    y_val = to_categorical(y_val, 5)\n",
        "    y_test = to_categorical(y_test, 5)\n",
        "    \n",
        "    # Assign to global variables\n",
        "    x_train, x_val, x_test = X_train, X_val, X_test\n",
        "    \n",
        "    print(f\"Dataset splits created:\")\n",
        "    print(f\"Train: {len(x_train)} samples\")\n",
        "    print(f\"Validation: {len(x_val)} samples\")\n",
        "    print(f\"Test: {len(x_test)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class mappings are now defined in the data loading cell above\n",
        "# This ensures they're available when needed for data processing\n",
        "\n",
        "# Display class information\n",
        "print(\"Class mappings:\")\n",
        "for short, full in class_short2full.items():\n",
        "    print(f\"  {short}: {full} (index: {class_dict[short]})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Part - ResNet Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the image size\n",
        "image_size = 224\n",
        "\n",
        "# Import necessary layers from TensorFlow Keras\n",
        "from tensorflow.keras.layers import Dropout, GlobalAveragePooling2D  \n",
        "from tensorflow.keras.applications import ResNet50  \n",
        "\n",
        "# Load the ResNet50 model pre-trained on ImageNet, excluding the top layer\n",
        "resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "# Set all ResNet50 layers as trainable\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = True  # Allow training of all layers in ResNet50\n",
        "\n",
        "# Import the Sequential API for model creation\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Initialize the Sequential model\n",
        "model = Sequential()\n",
        "# Add the ResNet50 model as a feature extractor (without the top layers)\n",
        "model.add(resnet)\n",
        "\n",
        "# Add Dropout layer to prevent overfitting (rate of 0.5)\n",
        "model.add(Dropout(0.5))\n",
        "# Add Global Average Pooling layer to reduce dimensionality of the output from ResNet50\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Flatten the pooled features for dense layer processing\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add BatchNormalization to standardize activations and improve training speed\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Add a dense layer with 512 neurons and ReLU activation\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# Add another dense layer with 256 neurons and ReLU activation\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "# Add a dense layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "# Add the final output layer with 5 neurons for 5 classes, using softmax for multi-class classification\n",
        "model.add(Dense(5, activation=\"softmax\"))\n",
        "# Display the summary of the model architecture\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the number of epochs for training\n",
        "epochs = 50\n",
        "\n",
        "# Import necessary callback modules from TensorFlow Keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Reduce learning rate on plateau callback: reduces LR when the validation loss plateaus\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    factor=0.5,  # Multiplies the learning rate by this factor when activated\n",
        "    patience=15,  # Number of epochs to wait for improvement before reducing LR\n",
        "    verbose=1,  # Print messages when learning rate is reduced\n",
        "    min_delta=0.0001,  # Minimum change to qualify as an improvement\n",
        "    cooldown=0,  # Number of epochs to wait before resuming normal learning rate\n",
        "    min_lr=1e-7,  # Minimum learning rate, prevents LR from going below this value\n",
        ")\n",
        "\n",
        "# EarlyStopping callback: stops training if validation loss doesn't improve for 'patience' epochs\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=epochs // 5,  # Number of epochs without improvement before stopping\n",
        "    restore_best_weights=True,  # Restore model weights from the epoch with the best performance\n",
        "    verbose=1,  # Print messages when early stopping is triggered\n",
        ")\n",
        "\n",
        "# ModelCheckpoint callback: saves the best model during training based on validation loss\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"/kaggle/working/resnet_ocular_disease_model.h5\", save_best_only=True)\n",
        "\n",
        "# List of callbacks to be used during model training\n",
        "callbacks = [checkpoint_cb, early_stopping_cb, reduce_lr]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the metrics to be tracked during training\n",
        "METRICS = [\n",
        "    tf.keras.metrics.AUC(name=\"auc\"),  # Area under the ROC curve metric\n",
        "    tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),  # Accuracy for categorical classification\n",
        "    tf.keras.metrics.AUC(name=\"prc\", curve=\"PR\"),  # Area under the Precision-Recall curve\n",
        "]\n",
        "\n",
        "# Add F1 score metric based on availability\n",
        "if HAS_TFA:\n",
        "    METRICS.append(tfa.metrics.F1Score(num_classes=5, average=\"weighted\", name=\"f1\"))\n",
        "    print(\"Using tensorflow_addons F1Score\")\n",
        "else:\n",
        "    METRICS.append(F1Score(num_classes=5, average=\"weighted\", name=\"f1\"))\n",
        "    print(\"Using custom F1Score implementation\")\n",
        "\n",
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and the specified metrics\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Adam optimizer with a learning rate\n",
        "    loss=\"categorical_crossentropy\",  # Loss function for multi-class classification\n",
        "    metrics=METRICS  # List of metrics to evaluate during training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training information\n",
        "print(\"Starting ResNet50 training...\")\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Validation samples: {len(x_val)}\")\n",
        "print(f\"Test samples: {len(x_test)}\")\n",
        "print(f\"Number of classes: 5\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Batch size: 32\")\n",
        "print(f\"Model will be saved as: resnet_ocular_disease_model.h5\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=(x_val, y_val), callbacks=callbacks)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final training accuracy: {history.history['acc'][-1]:.4f}\")\n",
        "print(f\"Final validation accuracy: {history.history['val_acc'][-1]:.4f}\")\n",
        "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
