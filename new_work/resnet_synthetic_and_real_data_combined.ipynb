{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing necessary libraries for ResNet combined synthetic and real data analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Setting the path to the directory containing preprocessed images\n",
        "DATA_PATH = \"/kaggle/input/ocular-disease-recognition-odir5k/preprocessed_images\"\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Loading the dataset from a CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(\"/kaggle/input/ocular-disease-recognition-odir5k/full_df.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping of short class labels to their full descriptive names\n",
        "class_short2full = {\n",
        "    \"G\": \"Glaucoma\",  # Short label 'G' represents Glaucoma\n",
        "    \"C\": \"Cataract\",  # Short label 'C' represents Cataract\n",
        "    \"A\": \"Age Related Macular Degeneration\",  # Short label 'A' represents ARMD\n",
        "    \"H\": \"Hypertension\",  # Short label 'H' represents Hypertension\n",
        "    \"M\": \"Myopia\"  # Short label 'M' represents Myopia\n",
        "}\n",
        "\n",
        "# Mapping of short class labels to numerical indices for machine learning models\n",
        "class_dict = {\n",
        "    \"G\": 0,  # Glaucoma is assigned index 0\n",
        "    \"C\": 1,  # Cataract is assigned index 1\n",
        "    \"A\": 2,  # ARMD is assigned index 2\n",
        "    \"H\": 3,  # Hypertension is assigned index 3\n",
        "    \"M\": 4   # Myopia is assigned index 4\n",
        "}\n",
        "\n",
        "# Data preprocessing and converting class labels\n",
        "data[\"class\"] = data[\"labels\"].apply(lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
        "\n",
        "CLASSES = [\"G\", \"C\", \"A\", \"H\", \"M\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1 - Real Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dictionary mapping each class to a list of image filenames\n",
        "dict_img_list = {\n",
        "    class_: data.loc[data[\"class\"] == class_][\"filename\"].values\n",
        "    for class_ in class_short2full.keys()\n",
        "}\n",
        "\n",
        "def create_dataset(img_list, class_label, max_images=None):\n",
        "    dataset = []\n",
        "    count = 0\n",
        "    \n",
        "    for img in img_list:\n",
        "        if max_images is not None and count >= max_images:\n",
        "            break\n",
        "        \n",
        "        image_path = os.path.join(DATA_PATH, img)\n",
        "        image = cv2.imread(image_path)\n",
        "        \n",
        "        if image is None:\n",
        "            continue\n",
        "        \n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        dataset.append([np.array(image), class_label])\n",
        "        count += 1\n",
        "    \n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an empty list to store the dataset\n",
        "dataset = []\n",
        "\n",
        "# Print message to indicate the start of the dataset building process\n",
        "print(\"START building real dataset\")\n",
        "\n",
        "# Loop through each class in the list of classes (CLASSES)\n",
        "for i, class_ in enumerate(CLASSES):\n",
        "    # Print the current class being processed along with its index\n",
        "    print(f\"[{i+1}/{len(CLASSES)}] adding {class_short2full[class_]} to dataset ...\")\n",
        "    \n",
        "    # Get the list of image filenames for the current class from dict_img_list\n",
        "    img_list = dict_img_list[class_]\n",
        "    # Retrieve the class label (index) for the current class\n",
        "    class_label = class_dict[class_]\n",
        "    # Call the create_dataset function to process the images and add them to the dataset\n",
        "    dataset += create_dataset(img_list, class_label, max_images=None)\n",
        "\n",
        "# Shuffle the dataset randomly to ensure varied order of images during training\n",
        "random.shuffle(dataset)\n",
        "\n",
        "print(\"COMPLETE building real dataset\")\n",
        "print(f\"Total real dataset size: {len(dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 - Synthetic Dataset Preparation (t-SNE Selected)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the t-SNE selected dataset (similar to the original approach)\n",
        "data_v2 = pd.read_csv(\"/kaggle/input/combined-tsne-new-1/combined_tsne_new-1.csv\")\n",
        "\n",
        "# Apply the same preprocessing\n",
        "data_v2[\"class\"] = data_v2[\"labels\"].apply(lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
        "\n",
        "# Create dictionary for synthetic data\n",
        "dict_img_list_v2 = {\n",
        "    class_: data_v2.loc[data_v2[\"class\"]==class_][\"filename\"].values\n",
        "    for class_ in class_short2full.keys()\n",
        "}\n",
        "\n",
        "# Build synthetic dataset\n",
        "dataset_v2 = []\n",
        "\n",
        "print(\"START building synthetic dataset\")\n",
        "for i, class_ in enumerate(CLASSES):\n",
        "    print(f\"[{i+1}/{len(CLASSES)}] adding {class_short2full[class_]} to synthetic dataset ...\")\n",
        "    img_list = dict_img_list_v2[class_]\n",
        "    class_label = class_dict[class_]\n",
        "    dataset_v2 += create_dataset(img_list, class_label, max_images=None)\n",
        "\n",
        "random.shuffle(dataset_v2)\n",
        "\n",
        "print(\"COMPLETE building synthetic dataset\")\n",
        "print(f\"Total synthetic dataset size: {len(dataset_v2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3 - Combining Real and Synthetic Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate the real and synthetic datasets\n",
        "combined_dataset = dataset + dataset_v2\n",
        "\n",
        "print(f\"Combined dataset size: {len(combined_dataset)}\")\n",
        "print(f\"Real data: {len(dataset)} samples\")\n",
        "print(f\"Synthetic data: {len(dataset_v2)} samples\")\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "random.shuffle(combined_dataset)\n",
        "\n",
        "# Parameters for data splitting\n",
        "image_size = 224\n",
        "num_classes = 5\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "\n",
        "# Preparing predictors and target variables\n",
        "train_x = np.array([i[0] for i in combined_dataset]).reshape(-1, image_size, image_size, 3)\n",
        "train_y = np.array([i[1] for i in combined_dataset])\n",
        "\n",
        "# Calculating the number of images for each split\n",
        "num_images = len(train_x)\n",
        "num_train = int(num_images * train_ratio)\n",
        "num_val = int(num_images * val_ratio)\n",
        "num_test = num_images - num_train - num_val\n",
        "\n",
        "# Splitting the dataset into train and remaining (validation + test)\n",
        "x_train, x_remaining, y_train, y_remaining = train_test_split(train_x, train_y, train_size=num_train, random_state=42)\n",
        "\n",
        "# Further splitting the remaining data into validation and test\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_remaining, y_remaining, test_size=num_test, random_state=42)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Print the number of images in each split\n",
        "print(f\"Number of images - Train: {len(x_train)}, Validation: {len(x_val)}, Test: {len(x_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4 - ResNet50 Model Training on Combined Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the image size\n",
        "image_size = 224\n",
        "\n",
        "# Import necessary layers from TensorFlow Keras\n",
        "from tensorflow.keras.layers import Dropout, GlobalAveragePooling2D  \n",
        "from tensorflow.keras.applications import ResNet50  \n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Load the ResNet50 model pre-trained on ImageNet, excluding the top layer\n",
        "resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "# Set all ResNet50 layers as trainable\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Import the Sequential API for model creation\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Initialize the Sequential model\n",
        "model = Sequential()\n",
        "# Add the ResNet50 model as a feature extractor (without the top layers)\n",
        "model.add(resnet)\n",
        "\n",
        "# Add Dropout layer to prevent overfitting (rate of 0.5)\n",
        "model.add(Dropout(0.5))\n",
        "# Add Global Average Pooling layer to reduce dimensionality of the output from ResNet50\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Flatten the pooled features for dense layer processing\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add BatchNormalization to standardize activations and improve training speed\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Add a dense layer with 512 neurons and ReLU activation\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# Add another dense layer with 256 neurons and ReLU activation\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "# Add a dense layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "# Add the final output layer with 5 neurons for 5 classes, using softmax for multi-class classification\n",
        "model.add(Dense(5, activation=\"softmax\"))\n",
        "\n",
        "# Display the summary of the model architecture\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the metrics to be tracked during training\n",
        "METRICS = [\n",
        "    tf.keras.metrics.AUC(name=\"auc\"),  # Area under the ROC curve metric\n",
        "    tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),  # Accuracy for categorical classification\n",
        "    tfa.metrics.F1Score(num_classes=5, average=\"weighted\", name=\"f1\"),  # F1 score, weighted average across all classes\n",
        "    tf.keras.metrics.AUC(name=\"prc\", curve=\"PR\"),  # Area under the Precision-Recall curve\n",
        "]\n",
        "\n",
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and the specified metrics\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Adam optimizer with a learning rate\n",
        "    loss=\"categorical_crossentropy\",  # Loss function for multi-class classification\n",
        "    metrics=METRICS  # List of metrics to evaluate during training\n",
        ")\n",
        "\n",
        "# Define the number of epochs for training\n",
        "epochs = 50\n",
        "\n",
        "# Import necessary callback modules from TensorFlow Keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Reduce learning rate on plateau callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    factor=0.5,  # Multiplies the learning rate by this factor when activated\n",
        "    patience=15,  # Number of epochs to wait for improvement before reducing LR\n",
        "    verbose=1,  # Print messages when learning rate is reduced\n",
        "    min_delta=0.0001,  # Minimum change to qualify as an improvement\n",
        "    cooldown=0,  # Number of epochs to wait before resuming normal learning rate\n",
        "    min_lr=1e-7,  # Minimum learning rate, prevents LR from going below this value\n",
        ")\n",
        "\n",
        "# EarlyStopping callback\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=epochs // 5,  # Number of epochs without improvement before stopping\n",
        "    restore_best_weights=True,  # Restore model weights from the epoch with the best performance\n",
        "    verbose=1,  # Print messages when early stopping is triggered\n",
        ")\n",
        "\n",
        "# ModelCheckpoint callback\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"/kaggle/working/resnet_combined_model.h5\", save_best_only=True)\n",
        "\n",
        "# List of callbacks to be used during model training\n",
        "callbacks = [checkpoint_cb, early_stopping_cb, reduce_lr]\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=(x_val, y_val), callbacks=callbacks)\n",
        "print(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the combined training data for future use\n",
        "np.save('/kaggle/working/x_train_combined.npy', x_train)\n",
        "np.save('/kaggle/working/y_train_combined.npy', y_train)\n",
        "\n",
        "np.save('/kaggle/working/x_val_combined.npy', x_val)\n",
        "np.save('/kaggle/working/y_val_combined.npy', y_val)\n",
        "\n",
        "np.save('/kaggle/working/x_test_combined.npy', x_test)\n",
        "np.save('/kaggle/working/y_test_combined.npy', y_test)\n",
        "\n",
        "print(\"Combined datasets saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
