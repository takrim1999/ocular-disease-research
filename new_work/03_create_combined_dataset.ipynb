{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Create Combined Dataset with t-SNE Selection\n",
        "\n",
        "This notebook combines real and synthetic data, applies t-SNE selection for dominant classes, and creates the missing `combined_tsne_new-1.csv` file.\n",
        "\n",
        "## Process:\n",
        "1. Load real ODIR data\n",
        "2. Load synthetic data (generated in previous step)\n",
        "3. Apply t-SNE selection to reduce dominant classes\n",
        "4. Combine real and synthetic data\n",
        "5. Create the combined dataset CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from PIL import Image\n",
        "\n",
        "print(\"Dependencies imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    # Dataset paths\n",
        "    odir_path = \"/kaggle/input/ocular-disease-recognition-odir5k/\"\n",
        "    synthetic_path = \"/kaggle/working/synthetic_data/\"\n",
        "    synthetic_metadata_path = \"/kaggle/working/synthetic_metadata.csv\"\n",
        "    \n",
        "    # Output path\n",
        "    output_path = \"/kaggle/working/combined_tsne_new-1.csv\"\n",
        "    \n",
        "    # Image parameters\n",
        "    img_size = 224\n",
        "    \n",
        "    # t-SNE parameters\n",
        "    tsne_perplexity = 30\n",
        "    tsne_n_iter = 1000\n",
        "    n_clusters = 50  # Number of clusters for dominant class reduction\n",
        "    \n",
        "    # Class mappings\n",
        "    classes = {\n",
        "        \"G\": \"Glaucoma\",\n",
        "        \"C\": \"Cataract\", \n",
        "        \"A\": \"Age Related Macular Degeneration\",\n",
        "        \"H\": \"Hypertension\",\n",
        "        \"M\": \"Myopia\"\n",
        "    }\n",
        "    \n",
        "    # Dominant classes that need t-SNE reduction (based on original paper)\n",
        "    dominant_classes = [\"G\", \"C\", \"A\"]  # These classes have more samples\n",
        "    \n",
        "config = Config()\n",
        "print(f\"Configuration loaded. Output will be saved to: {config.output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load real ODIR data\n",
        "print(\"Loading real ODIR data...\")\n",
        "df_real = pd.read_csv(os.path.join(config.odir_path, \"full_df.csv\"))\n",
        "df_real[\"class\"] = df_real[\"labels\"].apply(lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
        "\n",
        "# Filter for our target classes\n",
        "target_classes = list(config.classes.keys())\n",
        "df_real = df_real[df_real[\"class\"].isin(target_classes)]\n",
        "\n",
        "print(f\"Real data loaded: {len(df_real)} samples\")\n",
        "print(\"Real data distribution:\")\n",
        "print(df_real[\"class\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load synthetic data\n",
        "print(\"Loading synthetic data...\")\n",
        "if os.path.exists(config.synthetic_metadata_path):\n",
        "    df_synthetic = pd.read_csv(config.synthetic_metadata_path)\n",
        "    print(f\"Synthetic data loaded: {len(df_synthetic)} samples\")\n",
        "    print(\"Synthetic data distribution:\")\n",
        "    print(df_synthetic[\"disease_name\"].value_counts())\n",
        "else:\n",
        "    print(\"âŒ Synthetic data not found!\")\n",
        "    print(\"Please run the synthetic data generation first:\")\n",
        "    print(\"1. Run generate_synthetic_data.py\")\n",
        "    print(\"2. Or run the synthetic data generation notebook\")\n",
        "    df_synthetic = pd.DataFrame()  # Empty dataframe as fallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load and preprocess images for t-SNE\n",
        "def load_images_for_tsne(df, images_path, max_samples=None):\n",
        "    \"\"\"Load images and extract features for t-SNE\"\"\"\n",
        "    print(f\"Loading images for t-SNE analysis...\")\n",
        "    \n",
        "    features = []\n",
        "    labels = []\n",
        "    filenames = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        if max_samples and len(features) >= max_samples:\n",
        "            break\n",
        "            \n",
        "        # Load image\n",
        "        image_path = os.path.join(images_path, row[\"filename\"])\n",
        "        \n",
        "        if os.path.exists(image_path):\n",
        "            # Load and resize image\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = cv2.resize(image, (config.img_size, config.img_size))\n",
        "                \n",
        "                # Flatten image for t-SNE\n",
        "                features.append(image.flatten())\n",
        "                labels.append(row[\"class\"])\n",
        "                filenames.append(row[\"filename\"])\n",
        "    \n",
        "    return np.array(features), labels, filenames\n",
        "\n",
        "# Apply t-SNE selection to dominant classes\n",
        "def apply_tsne_selection(df, images_path, class_name, max_samples=300):\n",
        "    \"\"\"Apply t-SNE and clustering to select representative samples\"\"\"\n",
        "    print(f\"Applying t-SNE selection for {class_name}...\")\n",
        "    \n",
        "    # Get samples for this class\n",
        "    class_df = df[df[\"class\"] == class_name].copy()\n",
        "    \n",
        "    if len(class_df) == 0:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Limit samples for computational efficiency\n",
        "    if len(class_df) > max_samples:\n",
        "        class_df = class_df.sample(n=max_samples, random_state=42)\n",
        "    \n",
        "    # Load images\n",
        "    features, labels, filenames = load_images_for_tsne(class_df, images_path)\n",
        "    \n",
        "    if len(features) < 10:  # Need minimum samples for t-SNE\n",
        "        return class_df\n",
        "    \n",
        "    print(f\"  Loaded {len(features)} images for t-SNE\")\n",
        "    \n",
        "    # Apply t-SNE\n",
        "    print(f\"  Running t-SNE...\")\n",
        "    tsne = TSNE(n_components=2, perplexity=config.tsne_perplexity, \n",
        "                n_iter=config.tsne_n_iter, random_state=42)\n",
        "    tsne_results = tsne.fit_transform(features)\n",
        "    \n",
        "    # Apply K-means clustering\n",
        "    print(f\"  Applying K-means clustering...\")\n",
        "    kmeans = KMeans(n_clusters=min(config.n_clusters, len(features)//2), \n",
        "                   random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(tsne_results)\n",
        "    \n",
        "    # Select representative samples from each cluster\n",
        "    selected_indices = []\n",
        "    for cluster_id in np.unique(cluster_labels):\n",
        "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
        "        \n",
        "        # Select samples from this cluster (up to 3 per cluster)\n",
        "        n_select = min(3, len(cluster_indices))\n",
        "        selected_cluster_indices = np.random.choice(cluster_indices, \n",
        "                                                  size=n_select, \n",
        "                                                  replace=False)\n",
        "        selected_indices.extend(selected_cluster_indices)\n",
        "    \n",
        "    # Create filtered dataframe\n",
        "    selected_filenames = [filenames[i] for i in selected_indices]\n",
        "    filtered_df = class_df[class_df[\"filename\"].isin(selected_filenames)].copy()\n",
        "    \n",
        "    print(f\"  Selected {len(filtered_df)} samples from {len(class_df)} original samples\")\n",
        "    \n",
        "    return filtered_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply t-SNE selection to real data (dominant classes only)\n",
        "print(\"Applying t-SNE selection to real data...\")\n",
        "df_real_filtered = []\n",
        "\n",
        "for class_short in target_classes:\n",
        "    class_name = config.classes[class_short]\n",
        "    \n",
        "    if class_short in config.dominant_classes:\n",
        "        # Apply t-SNE selection for dominant classes\n",
        "        filtered_class_df = apply_tsne_selection(\n",
        "            df_real, \n",
        "            os.path.join(config.odir_path, \"preprocessed_images\"),\n",
        "            class_short\n",
        "        )\n",
        "    else:\n",
        "        # Keep all samples for minority classes\n",
        "        filtered_class_df = df_real[df_real[\"class\"] == class_short].copy()\n",
        "        print(f\"Keeping all {len(filtered_class_df)} samples for {class_name}\")\n",
        "    \n",
        "    df_real_filtered.append(filtered_class_df)\n",
        "\n",
        "# Combine filtered real data\n",
        "df_real_combined = pd.concat(df_real_filtered, ignore_index=True)\n",
        "print(f\"\\nReal data after t-SNE selection: {len(df_real_combined)} samples\")\n",
        "print(\"Distribution:\")\n",
        "print(df_real_combined[\"class\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process synthetic data (if available)\n",
        "if not df_synthetic.empty:\n",
        "    print(\"Processing synthetic data...\")\n",
        "    \n",
        "    # Rename columns to match real data format\n",
        "    df_synthetic_processed = df_synthetic.copy()\n",
        "    df_synthetic_processed = df_synthetic_processed.rename(columns={\n",
        "        'disease_name': 'class_name',\n",
        "        'class': 'class'\n",
        "    })\n",
        "    \n",
        "    # Ensure synthetic data has the same format as real data\n",
        "    synthetic_data = []\n",
        "    for idx, row in df_synthetic_processed.iterrows():\n",
        "        synthetic_data.append({\n",
        "            'filename': row['filename'],\n",
        "            'class': row['class'],\n",
        "            'labels': row['labels'],\n",
        "            'source': 'synthetic'\n",
        "        })\n",
        "    \n",
        "    df_synthetic_final = pd.DataFrame(synthetic_data)\n",
        "    print(f\"Synthetic data processed: {len(df_synthetic_final)} samples\")\n",
        "    \n",
        "else:\n",
        "    print(\"No synthetic data available - creating dataset with real data only\")\n",
        "    df_synthetic_final = pd.DataFrame()\n",
        "\n",
        "# Combine real and synthetic data\n",
        "print(\"Combining real and synthetic data...\")\n",
        "\n",
        "# Add source column to real data\n",
        "df_real_combined['source'] = 'real'\n",
        "\n",
        "# Combine datasets\n",
        "if not df_synthetic_final.empty:\n",
        "    df_combined = pd.concat([df_real_combined, df_synthetic_final], ignore_index=True)\n",
        "else:\n",
        "    df_combined = df_real_combined.copy()\n",
        "\n",
        "print(f\"Combined dataset: {len(df_combined)} samples\")\n",
        "print(\"Final distribution:\")\n",
        "print(df_combined[\"class\"].value_counts())\n",
        "print(\"\\nSource distribution:\")\n",
        "print(df_combined[\"source\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the combined dataset\n",
        "print(f\"Saving combined dataset to: {config.output_path}\")\n",
        "df_combined.to_csv(config.output_path, index=False)\n",
        "\n",
        "print(\"âœ… Combined dataset saved successfully!\")\n",
        "print(f\"ðŸ“ File: {config.output_path}\")\n",
        "print(f\"ðŸ“Š Total samples: {len(df_combined)}\")\n",
        "\n",
        "# Display sample of the combined dataset\n",
        "print(\"\\nðŸ“‹ Sample of combined dataset:\")\n",
        "print(df_combined.head(10))\n",
        "\n",
        "# Create summary statistics\n",
        "print(\"\\nðŸ“ˆ Dataset Summary:\")\n",
        "print(f\"Real data: {len(df_combined[df_combined['source'] == 'real'])} samples\")\n",
        "if not df_synthetic_final.empty:\n",
        "    print(f\"Synthetic data: {len(df_combined[df_combined['source'] == 'synthetic'])} samples\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Class distribution:\")\n",
        "for class_short, class_name in config.classes.items():\n",
        "    class_count = len(df_combined[df_combined['class'] == class_short])\n",
        "    print(f\"  {class_name} ({class_short}): {class_count} samples\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Combined dataset creation completed!\")\n",
        "print(f\"The missing file '/kaggle/input/combined-tsne-new-1/combined_tsne_new-1.csv' has been created!\")\n",
        "print(f\"You can now run the 'resnet_synthetic_and_real_data_combined.ipynb' notebook.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
