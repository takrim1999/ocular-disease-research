# ğŸ¨ Complete Guide: Generating Synthetic Ocular Disease Data

This guide shows you how to generate the missing synthetic data to replicate the full research methodology.

## ğŸ¯ **What We're Creating**

The missing file: `/kaggle/input/combined-tsne-new-1/combined_tsne_new-1.csv`

This file contains:
- **Real ODIR data** (with t-SNE selection for dominant classes)
- **Synthetic data** generated by Stable Diffusion
- **Combined dataset** ready for training ResNet50

## ğŸ“‹ **Step-by-Step Process**

### **Step 1: Generate Synthetic Images**

**Option A: Using Python Script (Recommended)**
```bash
# Run the synthetic data generation script
python generate_synthetic_data.py
```

**Option B: Using Jupyter Notebook**
1. Open `02_generate_synthetic_data_simple.ipynb`
2. Run all cells
3. This will generate 200 synthetic images per disease class

**What it creates:**
- `/kaggle/working/synthetic_data/` - Synthetic images by class
- `/kaggle/working/synthetic_metadata.csv` - Metadata for synthetic images

### **Step 2: Create Combined Dataset**

1. Open `03_create_combined_dataset.ipynb`
2. Run all cells
3. This will:
   - Load real ODIR data
   - Load synthetic data
   - Apply t-SNE selection to dominant classes
   - Combine real + synthetic data
   - Create the missing CSV file

**What it creates:**
- `/kaggle/working/combined_tsne_new-1.csv` - The missing dataset file

### **Step 3: Update Notebook Paths**

After creating the combined dataset, update the notebook paths:

```python
# In resnet_synthetic_and_real_data_combined.ipynb
# Change this line:
data_v2 = pd.read_csv("/kaggle/input/combined-tsne-new-1/combined_tsne_new-1.csv")

# To this:
data_v2 = pd.read_csv("/kaggle/working/combined_tsne_new-1.csv")
```

## âš™ï¸ **Requirements**

### **Hardware Requirements:**
- **GPU**: NVIDIA GPU with 8GB+ VRAM (recommended)
- **RAM**: 16GB+ system RAM
- **Storage**: 10GB+ free space

### **Software Requirements:**
```bash
pip install diffusers transformers accelerate torch torchvision
pip install pandas numpy matplotlib scikit-learn opencv-python
pip install pillow tqdm
```

## ğŸš€ **Quick Start (Kaggle)**

### **1. Upload Files to Kaggle:**
- Upload all notebooks and scripts to Kaggle
- Add ODIR dataset to your notebook

### **2. Generate Synthetic Data:**
```python
# Run in a Kaggle cell
!python generate_synthetic_data.py
```

### **3. Create Combined Dataset:**
```python
# Run in a Kaggle cell
!jupyter nbconvert --execute --to notebook 03_create_combined_dataset.ipynb
```

### **4. Run ResNet Training:**
Now you can run `resnet_synthetic_and_real_data_combined.ipynb`

## ğŸ“Š **Expected Output**

### **Synthetic Data Generation:**
- **Glaucoma**: 200 synthetic images
- **Cataract**: 200 synthetic images  
- **AMD**: 200 synthetic images
- **Hypertension**: 200 synthetic images
- **Myopia**: 200 synthetic images
- **Total**: 1,000 synthetic images

### **Combined Dataset:**
- **Real data**: ~1,200 samples (after t-SNE selection)
- **Synthetic data**: 1,000 samples
- **Total**: ~2,200 samples

## ğŸ”§ **Customization Options**

### **Adjust Number of Synthetic Images:**
```python
# In generate_synthetic_data.py
self.num_images_per_class = 500  # Increase for more synthetic data
```

### **Modify t-SNE Parameters:**
```python
# In 03_create_combined_dataset.ipynb
config.tsne_perplexity = 50  # Adjust perplexity
config.n_clusters = 100      # Adjust number of clusters
```

### **Change Prompts:**
```python
# In generate_synthetic_data.py
self.prompts = {
    "G": "your custom glaucoma prompt here",
    # ... other classes
}
```

## âš ï¸ **Important Notes**

### **Quality Considerations:**
- Synthetic data quality depends on prompt engineering
- Fine-tuning would improve quality but requires more resources
- Current approach uses pre-trained Stable Diffusion

### **Computational Requirements:**
- Generation takes 2-4 hours on GPU
- t-SNE processing takes 30-60 minutes
- Ensure sufficient disk space for intermediate files

### **Memory Management:**
- Use batch processing for large datasets
- Monitor GPU memory usage
- Consider reducing batch sizes if memory issues occur

## ğŸ› **Troubleshooting**

### **Common Issues:**

1. **Out of Memory Error:**
   ```python
   # Reduce batch size
   batch_size = 2  # Instead of 4
   ```

2. **Synthetic Data Not Found:**
   ```python
   # Check if synthetic data was generated
   import os
   print(os.listdir("/kaggle/working/synthetic_data/"))
   ```

3. **t-SNE Convergence Issues:**
   ```python
   # Reduce perplexity
   config.tsne_perplexity = 20
   ```

## ğŸ“ˆ **Performance Expectations**

### **Generation Time (NVIDIA T4 GPU):**
- **Synthetic images**: ~2-3 hours for 1,000 images
- **t-SNE processing**: ~30 minutes
- **Total time**: ~3-4 hours

### **Quality Metrics:**
- **Synthetic images**: Good visual quality, may lack fine details
- **t-SNE selection**: Maintains diversity while reducing redundancy
- **Combined dataset**: Balanced representation of all classes

## ğŸ‰ **Success Criteria**

You'll know it worked when:
1. âœ… Synthetic images are generated in `/kaggle/working/synthetic_data/`
2. âœ… Metadata CSV is created at `/kaggle/working/synthetic_metadata.csv`
3. âœ… Combined dataset is saved as `/kaggle/working/combined_tsne_new-1.csv`
4. âœ… `resnet_synthetic_and_real_data_combined.ipynb` runs without errors

## ğŸ“ **Need Help?**

If you encounter issues:
1. Check the error messages carefully
2. Verify all dependencies are installed
3. Ensure sufficient disk space and memory
4. Try reducing batch sizes or number of images
5. Check that ODIR dataset is properly loaded

The synthetic data generation is the most complex part, but once completed, you'll have the complete dataset needed for the full research replication!
