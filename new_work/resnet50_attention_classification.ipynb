{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from typing import Dict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Kaggle-style dataset root (keep this constant in code)\n",
        "DATASET_ROOT = \"/kaggle/input/ocular-disease-recognition-odir5k\"\n",
        "IMAGES_DIR = os.path.join(DATASET_ROOT, \"preprocessed_images\")\n",
        "CSV_PATH = os.path.join(DATASET_ROOT, \"full_df.csv\")\n",
        "\n",
        "# Image and training parameters\n",
        "IMAGE_SIZE: int = 224\n",
        "NUM_CLASSES: int = 5  # G, C, A, H, M\n",
        "BATCH_SIZE: int = 32\n",
        "EPOCHS: int = 35\n",
        "LEARNING_RATE: float = 2e-4\n",
        "\n",
        "# Class mappings (exclude 'D' and 'N')\n",
        "CLASS_SHORT_TO_FULL: Dict[str, str] = {\n",
        "    \"G\": \"Glaucoma\",\n",
        "    \"C\": \"Cataract\",\n",
        "    \"A\": \"AMD\",\n",
        "    \"H\": \"Hypertension\",\n",
        "    \"M\": \"Myopia\",\n",
        "}\n",
        "CLASS_TO_INDEX: Dict[str, int] = {k: i for i, k in enumerate(CLASS_SHORT_TO_FULL.keys())}\n",
        "INDEX_TO_CLASS: Dict[int, str] = {v: k for k, v in CLASS_TO_INDEX.items()}\n",
        "\n",
        "print(\"Using dataset root:\", DATASET_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CSV and filter classes\n",
        "raw_df = pd.read_csv(CSV_PATH)\n",
        "raw_df[\"class\"] = raw_df[\"labels\"].apply(lambda x: \" \".join(re.findall(\"[A-Za-z]+\", str(x))).strip())\n",
        "raw_df = raw_df[raw_df[\"class\"].isin(CLASS_SHORT_TO_FULL.keys())]\n",
        "raw_df[\"class_idx\"] = raw_df[\"class\"].map(CLASS_TO_INDEX)\n",
        "\n",
        "assert raw_df[\"filename\"].notna().all()\n",
        "assert raw_df[\"class_idx\"].notna().all()\n",
        "\n",
        "print(\"Class distribution (kept):\")\n",
        "print(raw_df[\"class\"].value_counts().sort_index())\n",
        "\n",
        "train_df, temp_df = train_test_split(raw_df, test_size=0.3, random_state=SEED, stratify=raw_df[\"class_idx\"])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"class_idx\"])\n",
        "\n",
        "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    print(name, df.shape, df[\"class\"].value_counts().sort_index().to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF Dataset pipeline\n",
        "\n",
        "def decode_image(path: tf.Tensor) -> tf.Tensor:\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE), antialias=True)\n",
        "    return image\n",
        "\n",
        "@tf.function\n",
        "def build_path(filename: tf.Tensor) -> tf.Tensor:\n",
        "    return tf.strings.join([IMAGES_DIR, \"/\", filename])\n",
        "\n",
        "@tf.function\n",
        "def one_hot(label_idx: tf.Tensor) -> tf.Tensor:\n",
        "    return tf.one_hot(label_idx, NUM_CLASSES, dtype=tf.float32)\n",
        "\n",
        "@tf.function\n",
        "def augment(image: tf.Tensor) -> tf.Tensor:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.05)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    return tf.clip_by_value(image, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def make_dataset(df: pd.DataFrame, training: bool) -> tf.data.Dataset:\n",
        "    filenames = df[\"filename\"].values.astype(str)\n",
        "    labels = df[\"class_idx\"].values.astype(np.int32)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "    ds = ds.shuffle(len(df), seed=SEED) if training else ds\n",
        "\n",
        "    def _load_map(fname, lbl):\n",
        "        path = build_path(fname)\n",
        "        img = decode_image(path)\n",
        "        if training:\n",
        "            img = augment(img)\n",
        "        return img, one_hot(lbl)\n",
        "\n",
        "    ds = ds.map(_load_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_df, training=True)\n",
        "val_ds = make_dataset(val_df, training=False)\n",
        "test_ds = make_dataset(test_df, training=False)\n",
        "\n",
        "train_ds, val_ds, test_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CBAM attention + ResNet50\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "\n",
        "def channel_attention(inputs: tf.Tensor, reduction_ratio: int = 8) -> tf.Tensor:\n",
        "    channels = inputs.shape[-1]\n",
        "    avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
        "    max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
        "\n",
        "    shared = layers.Dense(channels // reduction_ratio, activation=\"relu\", use_bias=True)\n",
        "    shared2 = layers.Dense(channels, activation=\"sigmoid\", use_bias=True)\n",
        "\n",
        "    avg_out = shared2(shared(avg_pool))\n",
        "    max_out = shared2(shared(max_pool))\n",
        "    scale = avg_out + max_out\n",
        "    return inputs * scale\n",
        "\n",
        "\n",
        "def spatial_attention(inputs: tf.Tensor, kernel_size: int = 7) -> tf.Tensor:\n",
        "    avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
        "    max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
        "    concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
        "    scale = layers.Conv2D(1, kernel_size=kernel_size, padding=\"same\", activation=\"sigmoid\")(concat)\n",
        "    return inputs * scale\n",
        "\n",
        "\n",
        "def cbam_block(inputs: tf.Tensor, reduction_ratio: int = 8, kernel_size: int = 7) -> tf.Tensor:\n",
        "    x = channel_attention(inputs, reduction_ratio=reduction_ratio)\n",
        "    x = spatial_attention(x, kernel_size=kernel_size)\n",
        "    return x\n",
        "\n",
        "\n",
        "base = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "base.trainable = True\n",
        "\n",
        "inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "x = base(inputs, training=True)\n",
        "x = cbam_block(x, reduction_ratio=8, kernel_size=7)\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile and train\n",
        "METRICS = [\n",
        "    tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    tf.keras.metrics.AUC(name=\"auc\", multi_label=False, num_thresholds=200),\n",
        "    tfa.metrics.F1Score(num_classes=NUM_CLASSES, average=\"weighted\", name=\"f1\"),\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "checkpoint_path = \"/kaggle/working/resnet50_cbam_best.h5\"\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor=\"val_acc\", mode=\"max\"),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1, min_lr=1e-6),\n",
        "    tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor=\"val_acc\", mode=\"max\"),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reusable confusion matrix plotting + evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "def plot_confusion_matrix(cm: np.ndarray, class_names: list, title: str):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "hist = history.history\n",
        "print({k: (float(np.max(v)) if 'loss' not in k else float(np.min(v))) for k, v in hist.items()})\n",
        "\n",
        "# Evaluate on test\n",
        "metrics = model.evaluate(test_ds, return_dict=True)\n",
        "print(\"Test metrics:\", metrics)\n",
        "\n",
        "# Predictions for CM and report\n",
        "y_true = []\n",
        "y_prob = []\n",
        "for images, labels in test_ds:\n",
        "    y_true.append(np.argmax(labels.numpy(), axis=1))\n",
        "    y_prob.append(model.predict(images, verbose=0))\n",
        "\n",
        "y_true = np.concatenate(y_true, axis=0)\n",
        "y_prob = np.concatenate(y_prob, axis=0)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [CLASS_SHORT_TO_FULL[k] for k in CLASS_SHORT_TO_FULL.keys()]\n",
        "plot_confusion_matrix(cm, class_names, title='Confusion Matrix (Test)')\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
        "print(report)\n",
        "\n",
        "try:\n",
        "    auc_ovr = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
        "    print(\"ROC-AUC (OvR):\", auc_ovr)\n",
        "except Exception as e:\n",
        "    print(\"ROC-AUC calculation skipped:\", e)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist['acc'], label='train acc')\n",
        "plt.plot(hist['val_acc'], label='val acc')\n",
        "plt.legend(); plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist['loss'], label='train loss')\n",
        "plt.plot(hist['val_loss'], label='val loss')\n",
        "plt.legend(); plt.title('Loss')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
